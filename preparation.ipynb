{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "\n",
    "from nltk.book import *\n",
    "from scipy import stats\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from collections import Counter\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "trainvar = pd.read_csv(\"../data/training_variants\")\n",
    "traintext = pd.read_csv(\"../data/training_text\", sep=\"\\|\\|\", header=None, skiprows=1, names=[\"ID\",\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos = trainvar.merge(traintext,on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordFilter(excluded, row):\n",
    "    filtered = [w for w in row if w not in excluded]\n",
    "    return filtered\n",
    "\n",
    "def lowerCaseArray(wordrow): \n",
    "    lowercased = [word.lower() for word in wordrow]\n",
    "    return lowercased  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "def wordStemmer(wordrow): \n",
    "    stemmed = [porter.stem(word) for word in wordrow]\n",
    "    return stemmed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_processing(df, clase):\n",
    "    data = {'matrix':[],'all':[]}\n",
    "    textos = df.loc[df['Class'] == clase]['Text']\n",
    "    interWordMatrix = []\n",
    "    interWordList = []\n",
    "    \n",
    "    for i in textos:\n",
    "        raw = i.decode(\"utf8\")   # pasar el texto a utf8\n",
    "        tokens = nltk.word_tokenize(raw)  # dividir texto en palabras \n",
    "        wordrow_lowercased = lowerCaseArray(tokens)  # pasar a minusculas\n",
    "        wordrow_nostopwords = wordFilter(stopwords,wordrow_lowercased)  # eliminar determinantes y demas\n",
    "        wordrow_stemmed = wordStemmer(wordrow_nostopwords) # quitar sufijos, afijos\n",
    "        interWordList.extend(wordrow_stemmed)  # todas las palabras \n",
    "        interWordMatrix.append(wordrow_stemmed) # \n",
    "    \n",
    "        \n",
    "    wordfreqs = nltk.FreqDist(interWordList)\n",
    "    hapaxes = wordfreqs.hapaxes()\n",
    "    for wordvector in interWordMatrix:\n",
    "        wordvector_nohapexes = wordFilter(hapaxes,wordvector)\n",
    "        data['matrix'].append(wordvector_nohapexes)\n",
    "        data['all'].extend(wordvector_nohapexes)\n",
    "    \n",
    "    print '---'\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished class\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-eda8b99a871d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mclase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0malldata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclase\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-79e365037f98>\u001b[0m in \u001b[0;36mtext_processing\u001b[0;34m(df, clase)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mhapaxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordfreqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mwordvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minterWordMatrix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mwordvector_nohapexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordFilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhapaxes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwordvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'matrix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordvector_nohapexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordvector_nohapexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-105762f0a383>\u001b[0m in \u001b[0;36mwordFilter\u001b[0;34m(excluded, row)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwordFilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexcluded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlowerCaseArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alldata = {}\n",
    "for clase in datos['Class'].unique():\n",
    "    alldata[clase] = text_processing(datos, clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('result.json', 'w') as fp:\n",
    "    json.dump(alldata, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
