{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import collections\n",
    "import nltk\n",
    "import re\n",
    "import random\n",
    "\n",
    "from nltk.book import *\n",
    "from scipy import stats\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from collections import Counter\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "trainvar = pd.read_csv(\"../data/training_variants\")\n",
    "traintext = pd.read_csv(\"../data/training_text\", sep=\"\\|\\|\", header=None, skiprows=1, names=[\"ID\",\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos = trainvar.merge(traintext,on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordFilter(excluded, row):\n",
    "    filtered = [w for w in row if w not in excluded]\n",
    "    return filtered\n",
    "\n",
    "def lowerCaseArray(wordrow): \n",
    "    lowercased = [word.lower() for word in wordrow]\n",
    "    return lowercased  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "def wordStemmer(wordrow): \n",
    "    stemmed = [porter.stem(word) for word in wordrow]\n",
    "    return stemmed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_processing(df, clase):\n",
    "    data = {'matrix':[],'all':[]}\n",
    "    textos = df.loc[df['Class'] == clase]['Text']\n",
    "    interWordMatrix = []\n",
    "    interWordList = []\n",
    "    \n",
    "    for i in textos:\n",
    "        raw = i.decode(\"utf8\")\n",
    "        tokens = nltk.word_tokenize(raw)      \n",
    "        wordrow_lowercased = lowerCaseArray(tokens)\n",
    "        wordrow_nostopwords = wordFilter(stopwords,wordrow_lowercased)\n",
    "        wordrow_stemmed = wordStemmer(wordrow_nostopwords)\n",
    "        interWordList.extend(wordrow_stemmed)\n",
    "        interWordMatrix.append(wordrow_stemmed)\n",
    "    \n",
    "    print 'finished class'\n",
    "        \n",
    "    wordfreqs = nltk.FreqDist(interWordList)\n",
    "    hapaxes = wordfreqs.hapaxes()\n",
    "    for wordvector in interWordMatrix:\n",
    "        wordvector_nohapexes = wordFilter(hapaxes,wordvector)\n",
    "        data['matrix'].append(wordvector_nohapexes)\n",
    "        data['all'].extend(wordvector_nohapexes)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished class\n"
     ]
    }
   ],
   "source": [
    "alldata = {}\n",
    "for clase in datos['Class'].unique():\n",
    "    alldata[clase] = text_processing(datos, clase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
